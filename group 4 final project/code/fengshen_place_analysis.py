"""
å°ç¥æ¼”ä¹‰åœ°ç‚¹å‡ºç°é¢‘ç‡åˆ†æå·¥å…·
åŠŸèƒ½ï¼šå¯¼å…¥CSVåŸæ–‡æ–‡ä»¶ï¼Œä½¿ç”¨jiebaåˆ†è¯ç»Ÿè®¡åœ°åå‡ºç°é¢‘ç‡
ä½œè€…ï¼šæ•°æ®åˆ†æå¸ˆ
æ—¥æœŸï¼š2024å¹´
"""

import pandas as pd
import jieba
import re
from collections import Counter
import argparse

def create_fengshen_place_dict(dict_path='fengshen_place_dict.txt'):
    """
    åˆ›å»ºå°ç¥æ¼”ä¹‰åœ°ç‚¹è‡ªå®šä¹‰è¯å…¸ï¼ˆç¹ä½“ä¸­æ–‡ï¼‰
    """
    # å°ç¥æ¼”ä¹‰ä¸­å¸¸è§çš„åœ°ç‚¹ï¼ˆç¹ä½“ä¸­æ–‡ï¼‰
    fengshen_places = [
        "å•†æœ 1000 nr", "è¥¿å‘¨ 800 nr", "ç´‚éƒ½ 600 nr", "è¥¿å² 1200 nr", "æœæ­Œ 1500 nr",
        "å­Ÿæ´¥ 500 nr", "å†€å· 400 nr", "é™³å¡˜é—œ 350 nr", "ä¹é¾å³¶ 300 nr", "é‡‘é³Œå³¶ 280 nr",
        "è‡¨æ½¼ 250 nr", "é»ƒæ²³ 450 nr", "æ¸­æ°´ 380 nr", "æ˜†ä¾–å±± 650 nr", "ç‰æ³‰å±± 220 nr",
        "é’å³°å±± 200 nr", "äº”å¤·å±± 180 nr", "å¤¾é¾å±± 160 nr", "é¾è™å±± 150 nr", "é¦–é™½å±± 140 nr",
        "ç‡•å±± 130 nr", "å²å±± 320 nr", "é„‚åŸ 120 nr", "æ½¼é—œ 280 nr", "è‡¨æ½¼é—œ 260 nr",
        "é»ƒæ²³æ¸¡å£ 180 nr", "å­Ÿæ´¥æ¸¡å£ 160 nr", "è¥¿å²åŸ 220 nr", "æœæ­ŒåŸ 250 nr", "æ‘˜æ˜Ÿæ¨“ 300 nr",
        "é¹¿å° 350 nr", "å¤ªå»Ÿ 180 nr", "é¾å¾·æ®¿ 150 nr", "ä¹é–“æ®¿ 140 nr", "å¥³å¨²å®® 280 nr",
        "éˆå° 220 nr", "è‚‰æ—é…’æ±  120 nr", "è–‘å¤ªå…¬åºœ 110 nr", "æ¯”å¹²åºœ 100 nr", "å¾®å­åºœ 90 nr",
        "ç®•å­åºœ 80 nr", "èå¤ªå¸«åºœ 150 nr", "é»ƒé£›è™åºœ 180 nr", "è˜‡è­·åºœ 120 nr", "å´‡ä¾¯è™åºœ 100 nr",
        "é„‚å´‡ç¦¹åºœ 90 nr", "å§¬æ˜Œåºœ 200 nr", "å§¬ç™¼åºœ 220 nr", "å“ªå’åºœ 150 nr", "æ¥Šæˆ©åºœ 140 nr",
        "é›·éœ‡å­åºœ 130 nr", "é»ƒå¤©åŒ–åºœ 120 nr", "é»ƒå¤©ç¥¿åºœ 110 nr", "é»ƒå¤©ç¥¥åºœ 100 nr", "åœŸè¡Œå­«åºœ 90 nr",
        "é„§ä¹å…¬åºœ 80 nr", "é„”æ–‡åŒ–åºœ 70 nr", "å¼µæ¡‚èŠ³åºœ 60 nr", "é­”å®¶å››å°‡åºœ 100 nr", "èä»²åºœ 150 nr",
        "ç”³å…¬è±¹åºœ 120 nr", "å§œå­ç‰™åºœ 200 nr", "å…ƒå§‹å¤©å°Šå®® 250 nr", "é€šå¤©æ•™ä¸»å®® 230 nr", "è€å­å®® 200 nr",
        "æ¥å¼•é“äººå»Ÿ 180 nr", "å‡†æé“äººå»Ÿ 170 nr", "åäºŒé‡‘ä»™åºœ 300 nr", "ç‰è™›å®® 400 nr", "ç¢§æ¸¸å®® 380 nr",
        "éˆéœ„æ®¿ 280 nr", "å—å¤©é–€ 320 nr", "ç‘¤æ±  250 nr", "èŸ æ¡ƒåœ’ 220 nr", "å…œç‡å®® 200 nr",
        "æœæ­Œçš‡å®® 300 nr", "è¥¿å²ç‹åºœ 280 nr", "å•†è»ç‡Ÿ 400 nr", "å‘¨è»ç‡Ÿ 450 nr", "å­Ÿæ´¥å¤§ç‡Ÿ 350 nr",
        "é»ƒæ²³å¤§ç‡Ÿ 320 nr", "æ½¼é—œå¤§ç‡Ÿ 300 nr", "è‡¨æ½¼å¤§ç‡Ÿ 280 nr", "å†€å·åŸ 250 nr", "é™³å¡˜é—œåŸ 230 nr",
        "è¥¿å²åŸé–€ 200 nr", "æœæ­ŒåŸé–€ 220 nr", "ä¹é¾å³¶æ´ 180 nr", "é‡‘é³Œå³¶æ´ 170 nr", "ç‰æ³‰å±±æ´ 150 nr",
        "é’å³°å±±æ± 140 nr", "äº”å¤·å±±æ± 130 nr", "å¤¾é¾å±±æ± 120 nr", "é¾è™å±±æ± 110 nr", "é¦–é™½å±±æ± 100 nr",
        "ç‡•å±±æ± 90 nr", "å²å±±æ± 80 nr", "æ˜†ä¾–å±±é ‚ 250 nr", "æ˜†ä¾–å±±è…³ 200 nr", "æ¸­æ°´ä¹‹æ¿± 220 nr",
        "é»ƒæ²³ä¹‹ç•” 200 nr", "å­Ÿæ´¥ä¹‹æ¿± 180 nr", "æœæ­ŒåŸå¤– 250 nr", "è¥¿å²åŸå¤– 230 nr", "å†€å·åŸå¤– 200 nr",
        "é™³å¡˜é—œå¤– 180 nr", "æ‘˜æ˜Ÿæ¨“é ‚ 150 nr", "é¹¿å°ä¹‹ä¸Š 180 nr", "å¤ªå»Ÿä¹‹å…§ 120 nr", "é¾å¾·æ®¿å…§ 100 nr",
        "ä¹é–“æ®¿å…§ 90 nr", "å¥³å¨²å®®å…§ 150 nr", "éˆå°ä¹‹ä¸Š 120 nr"
    ]
    
    # å†™å…¥è¯å…¸æ–‡ä»¶
    with open(dict_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(fengshen_places))
    
    print(f"âœ… æˆåŠŸåˆ›å»ºå°ç¥æ¼”ä¹‰åœ°ç‚¹è‡ªå®šä¹‰è¯å…¸: {dict_path}")
    print(f"ğŸ“š å…±åŒ…å« {len(fengshen_places)} ä¸ªåœ°ç‚¹è¯æ±‡")
    
    # è¿”å›åœ°ç‚¹åˆ—è¡¨
    place_list = [place.split()[0] for place in fengshen_places]
    return place_list, dict_path

def load_fengshen_data(file_path):
    """
    å¯¼å…¥å°ç¥æ¼”ä¹‰å…¨æ–‡CSVæ–‡ä»¶
    """
    try:
        df = pd.read_csv(file_path)
        print(f"ğŸ“– æˆåŠŸè¯»å–æ–‡ä»¶: {file_path}")
        print(f"ğŸ“Š æ•°æ®è§„æ¨¡: {df.shape[0]} ç« èŠ‚, {df.shape[1]} åˆ—")
        print(f"ğŸ·ï¸  åˆ—å: {df.columns.tolist()}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«full_textåˆ—
        if 'full_text' not in df.columns:
            raise ValueError("CSVæ–‡ä»¶ä¸­ç¼ºå°‘'full_text'åˆ—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
        
        return df
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        raise

def merge_all_text(df, text_column='full_text'):
    """
    åˆå¹¶æ‰€æœ‰ç« èŠ‚çš„æ–‡æœ¬å†…å®¹
    """
    all_text = ''
    for idx, row in df.iterrows():
        all_text += str(row[text_column]) + '\n'
    
    print(f"ğŸ“ æˆåŠŸåˆå¹¶æ‰€æœ‰æ–‡æœ¬")
    print(f"ğŸ“ æ–‡æœ¬æ€»é•¿åº¦: {len(all_text):,} å­—ç¬¦")
    print(f"ğŸ“š è¦†ç›–ç« èŠ‚æ•°: {len(df)} å›")
    
    return all_text

def preprocess_text(text):
    """
    æ–‡æœ¬é¢„å¤„ç†ï¼šå»é™¤æ ‡ç‚¹ç¬¦å·ã€ç‰¹æ®Šå­—ç¬¦ç­‰
    """
    # ä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼Œå»é™¤å…¶ä»–å­—ç¬¦
    pattern = re.compile(r'[^ä¸€-é¿¿]')
    cleaned_text = pattern.sub('', text)
    
    # å»é™¤å¤šä½™çš„ç©ºæ ¼
    cleaned_text = re.sub(r'\s+', '', cleaned_text)
    
    print(f"ğŸ§¹ æ–‡æœ¬é¢„å¤„ç†å®Œæˆ")
    print(f"ğŸ“Š é¢„å¤„ç†å‰: {len(text):,} å­—ç¬¦")
    print(f"ğŸ“Š é¢„å¤„ç†å: {len(cleaned_text):,} å­—ç¬¦")
    
    return cleaned_text

def segment_and_extract_places(text, place_list, jieba_instance):
    """
    å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶ç­›é€‰å‡ºåœ°ç‚¹è¯æ±‡
    """
    print(f"ğŸ” å¼€å§‹åˆ†è¯å’Œåœ°ç‚¹æå–...")
    
    # åˆ†è¯ï¼ˆç²¾ç¡®æ¨¡å¼ï¼‰
    words = jieba_instance.lcut(text, cut_all=False)
    print(f"âœ‚ï¸  æ€»åˆ†è¯æ•°: {len(words):,}")
    
    # ç­›é€‰å‡ºåœ°ç‚¹è¯æ±‡
    place_words = [word for word in words if word in place_list]
    print(f"ğŸ“ æå–å‡ºçš„åœ°ç‚¹è¯æ±‡æ€»æ•°: {len(place_words):,}")
    
    # å»é‡æŸ¥çœ‹æœ‰å¤šå°‘ä¸ªä¸åŒçš„åœ°ç‚¹è¢«è¯†åˆ«
    unique_places = list(set(place_words))
    print(f"ğŸ—ºï¸  è¯†åˆ«å‡ºçš„ä¸åŒåœ°ç‚¹æ•°é‡: {len(unique_places)}")
    
    return place_words, unique_places

def count_and_sort_places(place_words):
    """
    ç»Ÿè®¡åœ°ç‚¹è¯é¢‘å¹¶æŒ‰é¢‘ç‡æ’åº
    """
    # ç»Ÿè®¡è¯é¢‘
    place_counter = Counter(place_words)
    print(f"ğŸ“ˆ åœ°ç‚¹è¯é¢‘ç»Ÿè®¡å®Œæˆï¼Œå…±ç»Ÿè®¡ {len(place_counter)} ä¸ªåœ°ç‚¹")
    
    # æŒ‰é¢‘ç‡é™åºæ’åº
    sorted_places = sorted(place_counter.items(), key=lambda x: x[1], reverse=True)
    
    print(f"ğŸ† å‡ºç°é¢‘ç‡å‰10çš„åœ°ç‚¹:")
    for i, (place, count) in enumerate(sorted_places[:10], 1):
        print(f"  {i:2d}. {place}: {count:,} æ¬¡")
    
    return sorted_places, place_counter

def create_place_statistics_table(sorted_places, total_places_count, output_path='fengshen_place_statistics.csv'):
    """
    åˆ›å»ºåœ°ç‚¹ç»Ÿè®¡è¡¨æ ¼å¹¶ä¿å­˜ä¸ºCSVæ–‡ä»¶
    """
    # å‡†å¤‡ç»Ÿè®¡æ•°æ®
    statistics_data = []
    total_occurrences = sum([count for _, count in sorted_places])
    
    for rank, (place, count) in enumerate(sorted_places, 1):
        frequency_percent = (count / total_occurrences) * 100 if total_occurrences > 0 else 0
        cumulative_percent = (sum([c for _, c in sorted_places[:rank]]) / total_occurrences) * 100 if total_occurrences > 0 else 0
        
        # ç¡®å®šåœ°ç‚¹ç­‰çº§
        if rank <= 10:
            level = 'ä¸»è¦åœ°ç‚¹'
        elif rank <= 20:
            level = 'é‡è¦åœ°ç‚¹'
        else:
            level = 'æ¬¡è¦åœ°ç‚¹'
        
        statistics_data.append({
            'æ’å': rank,
            'åœ°ç‚¹åç§°': place,
            'å‡ºç°æ¬¡æ•°': count,
            'å‡ºç°é¢‘ç‡(%)': round(frequency_percent, 2),
            'ç´¯è®¡é¢‘ç‡(%)': round(cumulative_percent, 2),
            'ç­‰çº§': level
        })
    
    # åˆ›å»ºDataFrame
    df_statistics = pd.DataFrame(statistics_data)
    
    # ä¿å­˜ä¸ºCSVæ–‡ä»¶
    df_statistics.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ åœ°ç‚¹ç»Ÿè®¡è¡¨æ ¼å·²ä¿å­˜è‡³: {output_path}")
    
    # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
    print(f"ğŸ“Š ç»Ÿè®¡æ‘˜è¦:")
    print(f"- æ€»åœ°ç‚¹è¯æ±‡å‡ºç°æ¬¡æ•°: {total_occurrences:,}")
    print(f"- è¯†åˆ«å‡ºçš„ä¸åŒåœ°ç‚¹æ•°é‡: {len(df_statistics)}")
    
    main_places_count = sum(df_statistics[df_statistics['ç­‰çº§']=='ä¸»è¦åœ°ç‚¹']['å‡ºç°æ¬¡æ•°'])
    main_places_ratio = (main_places_count / total_occurrences * 100) if total_occurrences > 0 else 0
    print(f"- ä¸»è¦åœ°ç‚¹ï¼ˆå‰10åï¼‰å‡ºç°æ¬¡æ•°: {main_places_count:,} ({main_places_ratio:.1f}%)")
    
    important_places_count = sum(df_statistics[df_statistics['ç­‰çº§']=='é‡è¦åœ°ç‚¹']['å‡ºç°æ¬¡æ•°'])
    important_places_ratio = (important_places_count / total_occurrences * 100) if total_occurrences > 0 else 0
    print(f"- é‡è¦åœ°ç‚¹ï¼ˆå‰20åï¼‰å‡ºç°æ¬¡æ•°: {important_places_count:,} ({important_places_ratio:.1f}%)")
    
    return df_statistics

def main(input_csv_path, output_csv_path='fengshen_place_statistics.csv'):
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„åœ°ç‚¹ç»Ÿè®¡åˆ†ææµç¨‹
    """
    print("=" * 60)
    print("        å°ç¥æ¼”ä¹‰åœ°ç‚¹å‡ºç°é¢‘ç‡åˆ†æå·¥å…·        ")
    print("=" * 60)
    
    try:
        # 1. åˆ›å»ºè‡ªå®šä¹‰è¯å…¸
        place_list, dict_path = create_fengshen_place_dict()
        
        # 2. åˆå§‹åŒ–jiebaå¹¶åŠ è½½è¯å…¸
        jieba.load_userdict(dict_path)
        print(f"âœ… æˆåŠŸåŠ è½½è‡ªå®šä¹‰è¯å…¸åˆ°jieba")
        
        # 3. åŠ è½½CSVæ•°æ®
        df = load_fengshen_data(input_csv_path)
        
        # 4. åˆå¹¶æ–‡æœ¬
        all_text = merge_all_text(df)
        
        # 5. æ–‡æœ¬é¢„å¤„ç†
        cleaned_text = preprocess_text(all_text)
        
        # 6. åˆ†è¯å’Œåœ°ç‚¹æå–
        place_words, unique_places = segment_and_extract_places(cleaned_text, place_list, jieba)
        
        # 7. è¯é¢‘ç»Ÿè®¡å’Œæ’åº
        sorted_places, place_counter = count_and_sort_places(place_words)
        
        # 8. åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
        df_statistics = create_place_statistics_table(sorted_places, len(place_words), output_csv_path)
        
        print("\n" + "=" * 60)
        print("        åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜        ")
        print("=" * 60)
        
        return df_statistics
        
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼æˆ–è”ç³»æŠ€æœ¯æ”¯æŒ")
        return None

if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å°ç¥æ¼”ä¹‰åœ°ç‚¹å‡ºç°é¢‘ç‡åˆ†æå·¥å…·')
    parser.add_argument('input_file', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«full_textåˆ—ï¼‰')
    parser.add_argument('-o', '--output', default='fengshen_place_statistics.csv', 
                        help='è¾“å‡ºç»Ÿè®¡è¡¨æ ¼è·¯å¾„ï¼ˆé»˜è®¤ï¼šfengshen_place_statistics.csvï¼‰')
    
    args = parser.parse_args()
    
    # æ‰§è¡Œä¸»å‡½æ•°
    main(args.input_file, args.output)
